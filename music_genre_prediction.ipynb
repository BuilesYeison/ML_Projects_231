{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMsciraAJU00",
        "outputId": "f7899dd3-d6d4-4ab9-c10f-6d75522c9fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#@markdown --------------- \n",
        "#@markdown ## **⚠️❗ Ejecute esta celda para descargar Shoes dataset❗⚠️** \n",
        "#@markdown ### Esta celda creará la carpeta ```/content/genres_original```\n",
        "\n",
        "!pip install -qq gdown\n",
        "!gdown -qq \"https://drive.google.com/u/1/uc?id=189XUvtQu4E63VP_KmrMaVVVk243Dig1P&export=download\" -O /genres_original_short.zip\n",
        "!mkdir /content/genres_original/\n",
        "!unzip -qq /genres_original_short.zip -d /content/genres_original\n",
        "!rm -r /genres_original_short.zip\n",
        "print (\"Done!\")\n",
        "#@markdown ---------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import math\n",
        "import json \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time"
      ],
      "metadata": {
        "id": "aBs021gvEd-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/genres_original\"\n",
        "json_path = r\"data.json\"\n",
        "SAMPLE_RATE = 22050\n",
        "DURATION = 30\n",
        "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION"
      ],
      "metadata": {
        "id": "Py8I40aZEgWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_mfcc=13 \n",
        "n_fft=2048\n",
        "hop_length=512\n",
        "num_segments=10\n",
        "\n",
        "# Data storage dictionary\n",
        "data = {\n",
        "    \"mapping\": [],\n",
        "    \"mfcc\": [],\n",
        "    \"labels\": [],\n",
        "}\n",
        "samples_ps = int(SAMPLES_PER_TRACK/num_segments) # ps = per segment\n",
        "expected_vects_ps = math.ceil(samples_ps/hop_length)\n",
        "\n",
        "# loop through all the genres\n",
        "for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
        "    # ensuring not at root\n",
        "    if dirpath is not dataset_path:\n",
        "        # save the semantic label\n",
        "        dirpath_comp = dirpath.split(\"/\")\n",
        "        semantic_label = dirpath_comp[-1]\n",
        "        data[\"mapping\"].append(semantic_label)\n",
        "        print(f\"Processing: {semantic_label}\")\n",
        "        \n",
        "        # process files for specific genre\n",
        "        for f in filenames:\n",
        "            if(f==str(\"jazz.00054.wav\")):\n",
        "                # As librosa only read files <1Mb\n",
        "                continue\n",
        "            else:\n",
        "                # load audio file\n",
        "                file_path = os.path.join(dirpath, f)\n",
        "                signal,sr = librosa.load(file_path,sr=SAMPLE_RATE)\n",
        "                for s in range(num_segments):\n",
        "                    start_sample = int(samples_ps * s)\n",
        "                    finish_sample = int(start_sample + samples_ps)\n",
        "\n",
        "                    mfcc = librosa.feature.mfcc(y=signal[start_sample:finish_sample],\n",
        "                        sr=sr,\n",
        "                        n_mfcc=n_mfcc,\n",
        "                        hop_length=hop_length,\n",
        "                        n_fft=n_fft)\n",
        "\n",
        "                    mfcc = mfcc.T\n",
        "\n",
        "                    # store mfcc if it has expected length \n",
        "                    if len(mfcc)==expected_vects_ps:\n",
        "                        data[\"mfcc\"].append(mfcc.tolist())\n",
        "                        data[\"labels\"].append(i-1)\n",
        "                        print(f\"{file_path}, segment: {s+1}\")\n",
        "\n",
        "with open(json_path,\"w\") as f:\n",
        "    json.dump(data,f,indent=4)"
      ],
      "metadata": {
        "id": "ifg_vlfxEm07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707a923a-3229-4cd5-dc17-0e83a13d805d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: jazz\n",
            "/content/genres_original/jazz/jazz.00099.wav, segment: 1\n",
            "/content/genres_original/jazz/jazz.00099.wav, segment: 2\n",
            "/content/genres_original/jazz/jazz.00099.wav, segment: 3\n",
            "/content/genres_original/jazz/jazz.00099.wav, segment: 4\n",
            "/content/genres_original/jazz/jazz.00099.wav, segment: 5\n",
            "/content/genres_original/jazz/jazz.00099.wav, segment: 6\n",
            "/content/genres_original/jazz/jazz.00099.wav, segment: 7\n",
            "/content/genres_original/jazz/jazz.00099.wav, segment: 8\n",
            "/content/genres_original/jazz/jazz.00099.wav, segment: 9\n",
            "/content/genres_original/jazz/jazz.00099.wav, segment: 10\n",
            "Processing: classical\n",
            "/content/genres_original/classical/classical.00099.wav, segment: 1\n",
            "/content/genres_original/classical/classical.00099.wav, segment: 2\n",
            "/content/genres_original/classical/classical.00099.wav, segment: 3\n",
            "/content/genres_original/classical/classical.00099.wav, segment: 4\n",
            "/content/genres_original/classical/classical.00099.wav, segment: 5\n",
            "/content/genres_original/classical/classical.00099.wav, segment: 6\n",
            "/content/genres_original/classical/classical.00099.wav, segment: 7\n",
            "/content/genres_original/classical/classical.00099.wav, segment: 8\n",
            "/content/genres_original/classical/classical.00099.wav, segment: 9\n",
            "/content/genres_original/classical/classical.00099.wav, segment: 10\n",
            "Processing: metal\n",
            "/content/genres_original/metal/metal.00099.wav, segment: 1\n",
            "/content/genres_original/metal/metal.00099.wav, segment: 2\n",
            "/content/genres_original/metal/metal.00099.wav, segment: 3\n",
            "/content/genres_original/metal/metal.00099.wav, segment: 4\n",
            "/content/genres_original/metal/metal.00099.wav, segment: 5\n",
            "/content/genres_original/metal/metal.00099.wav, segment: 6\n",
            "/content/genres_original/metal/metal.00099.wav, segment: 7\n",
            "/content/genres_original/metal/metal.00099.wav, segment: 8\n",
            "/content/genres_original/metal/metal.00099.wav, segment: 9\n",
            "/content/genres_original/metal/metal.00099.wav, segment: 10\n",
            "Processing: disco\n",
            "/content/genres_original/disco/disco.00099.wav, segment: 1\n",
            "/content/genres_original/disco/disco.00099.wav, segment: 2\n",
            "/content/genres_original/disco/disco.00099.wav, segment: 3\n",
            "/content/genres_original/disco/disco.00099.wav, segment: 4\n",
            "/content/genres_original/disco/disco.00099.wav, segment: 5\n",
            "/content/genres_original/disco/disco.00099.wav, segment: 6\n",
            "/content/genres_original/disco/disco.00099.wav, segment: 7\n",
            "/content/genres_original/disco/disco.00099.wav, segment: 8\n",
            "/content/genres_original/disco/disco.00099.wav, segment: 9\n",
            "/content/genres_original/disco/disco.00099.wav, segment: 10\n",
            "Processing: reggae\n",
            "/content/genres_original/reggae/reggae.00099.wav, segment: 1\n",
            "/content/genres_original/reggae/reggae.00099.wav, segment: 2\n",
            "/content/genres_original/reggae/reggae.00099.wav, segment: 3\n",
            "/content/genres_original/reggae/reggae.00099.wav, segment: 4\n",
            "/content/genres_original/reggae/reggae.00099.wav, segment: 5\n",
            "/content/genres_original/reggae/reggae.00099.wav, segment: 6\n",
            "/content/genres_original/reggae/reggae.00099.wav, segment: 7\n",
            "/content/genres_original/reggae/reggae.00099.wav, segment: 8\n",
            "/content/genres_original/reggae/reggae.00099.wav, segment: 9\n",
            "/content/genres_original/reggae/reggae.00099.wav, segment: 10\n",
            "Processing: country\n",
            "/content/genres_original/country/country.00099.wav, segment: 1\n",
            "/content/genres_original/country/country.00099.wav, segment: 2\n",
            "/content/genres_original/country/country.00099.wav, segment: 3\n",
            "/content/genres_original/country/country.00099.wav, segment: 4\n",
            "/content/genres_original/country/country.00099.wav, segment: 5\n",
            "/content/genres_original/country/country.00099.wav, segment: 6\n",
            "/content/genres_original/country/country.00099.wav, segment: 7\n",
            "/content/genres_original/country/country.00099.wav, segment: 8\n",
            "/content/genres_original/country/country.00099.wav, segment: 9\n",
            "/content/genres_original/country/country.00099.wav, segment: 10\n",
            "Processing: rock\n",
            "/content/genres_original/rock/rock.00099.wav, segment: 1\n",
            "/content/genres_original/rock/rock.00099.wav, segment: 2\n",
            "/content/genres_original/rock/rock.00099.wav, segment: 3\n",
            "/content/genres_original/rock/rock.00099.wav, segment: 4\n",
            "/content/genres_original/rock/rock.00099.wav, segment: 5\n",
            "/content/genres_original/rock/rock.00099.wav, segment: 6\n",
            "/content/genres_original/rock/rock.00099.wav, segment: 7\n",
            "/content/genres_original/rock/rock.00099.wav, segment: 8\n",
            "/content/genres_original/rock/rock.00099.wav, segment: 9\n",
            "/content/genres_original/rock/rock.00099.wav, segment: 10\n",
            "Processing: hiphop\n",
            "/content/genres_original/hiphop/hiphop.00099.wav, segment: 1\n",
            "/content/genres_original/hiphop/hiphop.00099.wav, segment: 2\n",
            "/content/genres_original/hiphop/hiphop.00099.wav, segment: 3\n",
            "/content/genres_original/hiphop/hiphop.00099.wav, segment: 4\n",
            "/content/genres_original/hiphop/hiphop.00099.wav, segment: 5\n",
            "/content/genres_original/hiphop/hiphop.00099.wav, segment: 6\n",
            "/content/genres_original/hiphop/hiphop.00099.wav, segment: 7\n",
            "/content/genres_original/hiphop/hiphop.00099.wav, segment: 8\n",
            "/content/genres_original/hiphop/hiphop.00099.wav, segment: 9\n",
            "/content/genres_original/hiphop/hiphop.00099.wav, segment: 10\n",
            "Processing: blues\n",
            "/content/genres_original/blues/blues.00099.wav, segment: 1\n",
            "/content/genres_original/blues/blues.00099.wav, segment: 2\n",
            "/content/genres_original/blues/blues.00099.wav, segment: 3\n",
            "/content/genres_original/blues/blues.00099.wav, segment: 4\n",
            "/content/genres_original/blues/blues.00099.wav, segment: 5\n",
            "/content/genres_original/blues/blues.00099.wav, segment: 6\n",
            "/content/genres_original/blues/blues.00099.wav, segment: 7\n",
            "/content/genres_original/blues/blues.00099.wav, segment: 8\n",
            "/content/genres_original/blues/blues.00099.wav, segment: 9\n",
            "/content/genres_original/blues/blues.00099.wav, segment: 10\n",
            "Processing: pop\n",
            "/content/genres_original/pop/pop.00099.wav, segment: 1\n",
            "/content/genres_original/pop/pop.00099.wav, segment: 2\n",
            "/content/genres_original/pop/pop.00099.wav, segment: 3\n",
            "/content/genres_original/pop/pop.00099.wav, segment: 4\n",
            "/content/genres_original/pop/pop.00099.wav, segment: 5\n",
            "/content/genres_original/pop/pop.00099.wav, segment: 6\n",
            "/content/genres_original/pop/pop.00099.wav, segment: 7\n",
            "/content/genres_original/pop/pop.00099.wav, segment: 8\n",
            "/content/genres_original/pop/pop.00099.wav, segment: 9\n",
            "/content/genres_original/pop/pop.00099.wav, segment: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data.json\",\"r\") as f:\n",
        "    data = json.load(f)\n",
        "    # Convert list to numpy arrays\n",
        "    X = np.array(data[\"mfcc\"])\n",
        "    y = np.array(data[\"labels\"])\n",
        "      \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2)\n",
        "    X_train = X_train[..., np.newaxis]\n",
        "    X_val = X_val[..., np.newaxis]\n",
        "    X_test = X_test[..., np.newaxis]"
      ],
      "metadata": {
        "id": "PuSmHUFap7WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwlnd8ih2dF1",
        "outputId": "bb0b9f7d-2260-4aaa-a725-bcf2b5cadda0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 130, 13, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.Conv2d(32, 32, kernel_size=2, padding=0),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.Conv2d(32, 16, kernel_size=1, padding=0),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=1, stride=2, padding=0),\n",
        "    nn.BatchNorm2d(16),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(16 * 7 * 7, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(64, 10),\n",
        "    nn.Softmax(dim=1)\n",
        ")"
      ],
      "metadata": {
        "id": "fXjbE5B8zf9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
        "model = model.double()\n",
        "model.cuda()\n",
        "loss.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bne8K2r0E60",
        "outputId": "7c708ccf-d3b1-4274-8681-ca807f564154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "batch_size = 32\n",
        "losses = []\n",
        "\n",
        "start_time = time.time()\n",
        "progress = tqdm(range( epochs ), ncols=110)\n",
        "\n",
        "for epoch in progress:\n",
        "  #\n",
        "  batch_losses = 0\n",
        "  \n",
        "  for batch_i in range(0, len(X_train) , batch_size):\n",
        "    #\n",
        "    batch_X = X_train[ batch_i : batch_i+batch_size ]\n",
        "    batch_y = y_train[ batch_i : batch_i+batch_size ]\n",
        "    batch_X = torch.from_numpy( batch_X ).cuda()\n",
        "    batch_y = torch.from_numpy( batch_y ).cuda()\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform forward pass\n",
        "    predictions = model( batch_X )\n",
        "\n",
        "    # Compute loss\n",
        "    batch_loss = loss( predictions , batch_y )\n",
        "\n",
        "    # Perform backward pass\n",
        "    batch_loss.backward()\n",
        "\n",
        "    # Optimize parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    ## Save stats\n",
        "    batch_losses += batch_loss.item()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "  progress.set_description(\"Epoch [%d/%d] [Loss: %f] time: %3f\" % (epoch, epochs,\n",
        "                                                                   batch_loss.item(),\n",
        "                                                                   elapsed_time))\n",
        "  \n",
        "  losses.append(batch_losses/(len(X_train)/batch_size))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "Ck-OMU7S0E3b",
        "outputId": "3bc0babe-8d30-4f70-f91d-9c362a76da85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|                                                                                  | 0/40 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8d17c1235d8f>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Perform forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbatch_X\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 1, 3, 3], expected input[32, 130, 13, 1] to have 1 channels, but got 130 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import optimizers\n",
        "import tensorflow.keras as keras"
      ],
      "metadata": {
        "id": "b8DXQskn3SDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), activation = \"relu\", input_shape = (130, 13, 1)))\n",
        "model.add(MaxPool2D((3, 3), strides=(2, 2), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation = \"relu\"))\n",
        "model.add(MaxPool2D((3, 3), strides=(2, 2), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(32, (2, 2), activation = \"relu\"))\n",
        "model.add(MaxPool2D((2, 2), strides=(2, 2), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(16, (1, 1), activation = \"relu\"))\n",
        "model.add(MaxPool2D((1, 1), strides=(2, 2), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H282OfKm3iuC",
        "outputId": "74f886e7-3277-4613-f0b5-865444db6128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 128, 11, 64)       640       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 64, 6, 64)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 64, 6, 64)        256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 62, 4, 32)         18464     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 31, 2, 32)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 31, 2, 32)        128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 30, 1, 32)         4128      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 15, 1, 32)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 15, 1, 32)        128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 15, 1, 16)         528       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 8, 1, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 8, 1, 16)         64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33,242\n",
            "Trainable params: 32,954\n",
            "Non-trainable params: 288\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "adam = optimizers.Adam(lr=1e-4)\n",
        "model.compile(optimizer=adam,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "hist = model.fit(X_train, y_train,\n",
        "                 validation_data = (X_val, y_val),\n",
        "                 epochs = 40,\n",
        "                 batch_size = 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBrKt_Hr4Yao",
        "outputId": "f35f25da-7dea-4584-f088-fe15e77dca12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "2/2 [==============================] - 8s 413ms/step - loss: 2.7232 - accuracy: 0.0938 - val_loss: 5.7440 - val_accuracy: 0.2500\n",
            "Epoch 2/40\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 2.1086 - accuracy: 0.3125 - val_loss: 6.0813 - val_accuracy: 0.2500\n",
            "Epoch 3/40\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 1.6793 - accuracy: 0.3594 - val_loss: 6.1579 - val_accuracy: 0.2500\n",
            "Epoch 4/40\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.3308 - accuracy: 0.5469 - val_loss: 6.0862 - val_accuracy: 0.2500\n",
            "Epoch 5/40\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.1912 - accuracy: 0.6875 - val_loss: 5.8065 - val_accuracy: 0.2500\n",
            "Epoch 6/40\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.0588 - accuracy: 0.7656 - val_loss: 5.3898 - val_accuracy: 0.2500\n",
            "Epoch 7/40\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.9458 - accuracy: 0.8281 - val_loss: 5.0502 - val_accuracy: 0.2500\n",
            "Epoch 8/40\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.8841 - accuracy: 0.7812 - val_loss: 4.7573 - val_accuracy: 0.2500\n",
            "Epoch 9/40\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.7748 - accuracy: 0.7969 - val_loss: 4.6046 - val_accuracy: 0.2500\n",
            "Epoch 10/40\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.5950 - accuracy: 0.9375 - val_loss: 4.5983 - val_accuracy: 0.2500\n",
            "Epoch 11/40\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.4879 - accuracy: 0.9531 - val_loss: 4.6219 - val_accuracy: 0.2500\n",
            "Epoch 12/40\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.4087 - accuracy: 0.9844 - val_loss: 4.6946 - val_accuracy: 0.2500\n",
            "Epoch 13/40\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.4164 - accuracy: 0.9531 - val_loss: 4.7268 - val_accuracy: 0.2500\n",
            "Epoch 14/40\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.3393 - accuracy: 0.9688 - val_loss: 4.6567 - val_accuracy: 0.2500\n",
            "Epoch 15/40\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.2847 - accuracy: 1.0000 - val_loss: 4.5535 - val_accuracy: 0.2500\n",
            "Epoch 16/40\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.3339 - accuracy: 0.9844 - val_loss: 4.4291 - val_accuracy: 0.2500\n",
            "Epoch 17/40\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.2480 - accuracy: 1.0000 - val_loss: 4.3131 - val_accuracy: 0.2500\n",
            "Epoch 18/40\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.2087 - accuracy: 1.0000 - val_loss: 4.1886 - val_accuracy: 0.2500\n",
            "Epoch 19/40\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.2027 - accuracy: 1.0000 - val_loss: 4.0753 - val_accuracy: 0.2500\n",
            "Epoch 20/40\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1899 - accuracy: 1.0000 - val_loss: 3.9917 - val_accuracy: 0.2500\n",
            "Epoch 21/40\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.1573 - accuracy: 1.0000 - val_loss: 3.9089 - val_accuracy: 0.2500\n",
            "Epoch 22/40\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1750 - accuracy: 1.0000 - val_loss: 3.8011 - val_accuracy: 0.2500\n",
            "Epoch 23/40\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.1700 - accuracy: 0.9844 - val_loss: 3.7164 - val_accuracy: 0.2500\n",
            "Epoch 24/40\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.1170 - accuracy: 1.0000 - val_loss: 3.6163 - val_accuracy: 0.2500\n",
            "Epoch 25/40\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1109 - accuracy: 1.0000 - val_loss: 3.5195 - val_accuracy: 0.2500\n",
            "Epoch 26/40\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0998 - accuracy: 1.0000 - val_loss: 3.4230 - val_accuracy: 0.2500\n",
            "Epoch 27/40\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0922 - accuracy: 1.0000 - val_loss: 3.3184 - val_accuracy: 0.2500\n",
            "Epoch 28/40\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0799 - accuracy: 1.0000 - val_loss: 3.2408 - val_accuracy: 0.2500\n",
            "Epoch 29/40\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0730 - accuracy: 1.0000 - val_loss: 3.1575 - val_accuracy: 0.2500\n",
            "Epoch 30/40\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.1501 - accuracy: 0.9844 - val_loss: 3.0211 - val_accuracy: 0.2500\n",
            "Epoch 31/40\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0706 - accuracy: 1.0000 - val_loss: 2.8971 - val_accuracy: 0.2500\n",
            "Epoch 32/40\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0841 - accuracy: 1.0000 - val_loss: 2.7863 - val_accuracy: 0.2500\n",
            "Epoch 33/40\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0821 - accuracy: 1.0000 - val_loss: 2.7109 - val_accuracy: 0.2500\n",
            "Epoch 34/40\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0675 - accuracy: 1.0000 - val_loss: 2.6599 - val_accuracy: 0.2500\n",
            "Epoch 35/40\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0843 - accuracy: 1.0000 - val_loss: 2.6445 - val_accuracy: 0.2500\n",
            "Epoch 36/40\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0433 - accuracy: 1.0000 - val_loss: 2.6489 - val_accuracy: 0.2500\n",
            "Epoch 37/40\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0759 - accuracy: 1.0000 - val_loss: 2.6535 - val_accuracy: 0.2500\n",
            "Epoch 38/40\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0501 - accuracy: 1.0000 - val_loss: 2.6651 - val_accuracy: 0.2500\n",
            "Epoch 39/40\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0591 - accuracy: 0.9844 - val_loss: 2.6581 - val_accuracy: 0.2500\n",
            "Epoch 40/40\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0592 - accuracy: 1.0000 - val_loss: 2.6108 - val_accuracy: 0.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"Test accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "2egbzIA54tqG",
        "outputId": "0c81cead-b522-4471-ef96-88257c0732fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 163ms/step - loss: 3.2238 - accuracy: 0.2500\n",
            "Test accuracy: 0.25\n"
          ]
        }
      ]
    }
  ]
}